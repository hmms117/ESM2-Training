model:
  model_name: "facebook/esm2_t30_150M_UR50D"
  max_position_embeddings: 4098          # Maximum sequence length (including BOS/EOS tokens)
  max_batch_size: 8196                   # Maximum number of tokens per batch for pre-batching
  use_fa: true                           
  emb_layer_norm_before: true            # Apply LayerNorm before embedding projection
  num_layers: 30                         
  hidden_size: 640                       
  intermediate_size: 2560                
  num_attention_heads: 20                

training:
  gradient_accumulation_steps: 64
  learning_rate: 0.0004
  optimizer:
    type: muon
    beta_1: 0.99
    beta_2: 0.98
    epsilon: 1e-12
    weight_decay: 0.01
  gradient_clipping: 0.5
  lr_scheduler: "linear"
  evaluation_strategy: "steps"
  total_steps: 120000 # Total optimization steps (after accumulation) #125k steps would be ~45 epochs (like ESM-2) with our dataset size
  warmup_steps: 2000
  mlm_probability: 0.25 # Masking probability (could be 0.2 or 0.15 from "Training Compute-Optimal Protein Language Models" paper) maybe split the difference and call it 0.2
  logging_steps: 100
  eval_steps: 10000
  save_steps: 10000
  output_dir: "./runs/pLM120K_NL"
  mixed_precision: "fp16"
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 4
  dataloader_pin_memory: true
  seed: 100

data:
  chunk_size: 1000000
  default_tmp_dir: "data/tmp_chunks"
  default_shard_size: 25000